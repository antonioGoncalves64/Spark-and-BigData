{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/spark.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "# Introduction Spark and Pyspark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Function in Lambda Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Taken literally, an anonymous function is a function without a name. In Python, an anonymous function is created with the lambda keyword. More loosely, it may or not be assigned a name. \n",
    "Lambda functions can have any number of arguments but only one expression. The expression is evaluated and returned. Lambda functions can be used wherever function objects are required.\n",
    "\n",
    "**Syntax: lambda arguments: expression**\n",
    "\n",
    "1. This function can have any number of arguments but only one expression, which is evaluated and returned.\n",
    "1. One is free to use lambda functions wherever function objects are required.\n",
    "1. You need to keep in your knowledge that lambda functions are syntactically restricted to a single expression.\n",
    "1. It has various uses in particular fields of programming, besides other types of expressions in functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a two-argument anonymous function defined with lambda but not bound to a variable. The lambda is not given a name.\n",
    "The lambda is not given a name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda x, y: x + y)(2, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a one-argument anonymous function defined with lambda but  bound to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Multiply an element by 2 \n",
    "double = lambda x: x * 2\n",
    "\n",
    "print(double(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is nearly the same as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def double(x):\n",
    "   return x * 2\n",
    "\n",
    "double(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, we generally use it as an argument to a higher-order function (a function that takes in other functions as arguments). Lambda functions are used along with built-in functions like filter(), map() etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Using filter function\n",
    "\n",
    "The filter() function in Python takes in a function and a list as arguments.\n",
    "The function is called with all the items in the list and a new list is returned which contains items for which the function evaluates to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 4, 6, 8, 11, 3, 12]\n",
      "[8, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "# Program to filter out only the items > than 6  from a list\n",
    "my_list = [1, 5, 4, 6, 8, 11, 3, 12]\n",
    "print(my_list)\n",
    "\n",
    "new_list = list(filter(lambda x: (x > 6) , my_list))\n",
    "\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Using map function\n",
    "\n",
    "The map() function in Python takes in a function and a list.\n",
    "The function is called with all the items in the list and a new list is returned which contains items returned by that function for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 10, 8, 12, 16, 22, 6, 24]\n"
     ]
    }
   ],
   "source": [
    "# Program to double each item in a list using map()\n",
    "\n",
    "my_list = [1, 5, 4, 6, 8, 11, 3, 12]\n",
    "\n",
    "new_list = list(map(lambda x: x * 2 , my_list))\n",
    "\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Using reduce function\n",
    "\n",
    "The reduce() function in Python is a function that implements a mathematical technique called folding or reduction. \n",
    "reduce() is useful when you need to apply a function to an iterable and reduce it to a single cumulative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "lst = [1, 2, 3, 4, 5]\n",
    "reduce(lambda x, y: x + y, lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of a Lambda Function in Python\n",
    "\n",
    "* Pros\n",
    "  * It’s an ideal choice for evaluating a single expression that is supposed to be evaluated only once.\n",
    "  * It can be invoked as soon as it is defined.\n",
    "  * Its syntax is more compact in comparison to a corresponding normal function.\n",
    "  * It can be passed as a parameter to a higher-order function, like filter(), map(), and reduce().\n",
    "\n",
    "* Cons\n",
    "  * It can’t perform multiple expressions.\n",
    "  * It can easily become cumbersome, for example when it includes an if-elif-…-else cycle.\n",
    "  * It can’t contain any variable assignements (e.g., lambda x: x=0 will throw a SyntaxError).\n",
    "  * We can’t provide a docstring to a lambda function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Programming in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizable and non-parallelizable tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Some tasks are easily parallelizable while others inherently aren’t. \n",
    "However, it might not always be immediately apparent that a task is parallelizable.\n",
    "Let us consider the following piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 10\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3, 4] # input\n",
    "\n",
    "sum = 0 # Initialize output\n",
    "\n",
    "for e in x:\n",
    "  sum = sum + e # Add each element to the output variable\n",
    "\n",
    "print(\"Sum:\", sum) # Print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/serial.png\" alt=\"serial\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are performing the loops in a serial way in the code above, nothing avoids us from performing this calculation in parallel. \n",
    "The following example shows that parts of the computations can be done independently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "\n",
    "x = [1, 2, 3, 4] #input\n",
    "\n",
    "chunk1 = x[:2]\n",
    "chunk2 = x[2:]\n",
    "\n",
    "sum_1 = reduce(lambda x, y: x + y, chunk1)\n",
    "sum_2 = reduce(lambda x, y: x + y, chunk2)\n",
    "\n",
    "result = sum_1 + sum_2\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/serial2.png\" alt=\"serial\" width=\"900\"/>\n",
    "\n",
    "The technique for parallelising sums like this is called chunking. \n",
    "\n",
    "There is a subclass of algorithms where the subtasks are completely independent. \n",
    "These kinds of algorithms are known as embarrassingly parallel, or  parallel algorithms. An example of this kind of problem is squaring each element in a list, which can be done like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:  [1, 2, 3, 4]\n",
      "out: [1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3, 4] #input\n",
    "\n",
    "print(\"in: \", x)\n",
    "\n",
    "y = [n**2 for n in x]\n",
    "\n",
    "print(\"out:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each task of squaring a number is independent of all the other elements in the list. It is important to know that some tasks are fundamentally non-parallelizable. An example of such an inherently serial algorithm could be the computation of the fibonacci sequence using the formula Fn=Fn-1 + Fn-2.Each output here depends on the outputs of the two previous loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Parallelised Pea Soup\n",
    "\n",
    "**We have the following recipe:**\n",
    "\n",
    "* (01 min) Pour water into a soup pan, add the split peas and bay leaf and bring it to boil.\n",
    "* (60 min) Remove any foam using a skimmer and let it simmer under a lid for about 60 minutes.\n",
    "* (15 min) Clean and chop the leek, celeriac, onion, carrot and potato.\n",
    "* (20 min) Remove the bay leaf, add the vegetables and simmer for 20 more minutes. Stir the soup occasionally.\n",
    "* (1 day ) Leave the soup for one day. Reheat before serving and add a sliced smoked sausage (vegetarian options are also welcome). Season with pepper and salt.\n",
    "\n",
    "**Imagine you’re cooking alone.**\n",
    "\n",
    "1. Can you identify potential for parallelisation in this recipe?\n",
    "1. And what if you are cooking with the help of a friend help? Is the soup done any faster?\n",
    "1. Draw a dependency diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "    \n",
    "1. You can cut vegetables while simmering the split peas.\n",
    "1. If you have help, you can parallelize cutting vegetables further.\n",
    "1. There are two ‘workers’: the cook and the stove.\n",
    "\n",
    "\n",
    "<img src=\"images/ParallelisedPeaSoup.png\" alt=\"ParallelisedPeaSoup\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Shared vs. Distributed memory\n",
    "\n",
    "When a program runs on multiple cores with access to the same physical memory, it can use a shared memory design. In shared memory programming, the computation occurs within a single multithreaded process, and each thread within the process has access to memory in the same virtual address space. Communication among threads is efficient because any changes to shared memory are immediately visible to all other threads in that process.\n",
    "\n",
    "In contrast, when a program uses multiple processes, such that computational tasks do not share memory, its processes cannot communicate by writing updated values directly to memory. This situation requires a distributed memory design. In distributed memory programming, each process owns part of the data, and other processes must send a message to the owner in order to update that part of the data. The communication could be between processes running on the same node, or between processes running on different nodes in a cluster, but the underlying communication model is the same. Interface standards like Message-Passing Interface (MPI) facilitate distributed memory programming for cluster systems.\n",
    "\n",
    "In the following figure, we see a typical shared-memory architecture where four processors (the four CPU boxes in the following diagram) can all access the same memory address space (that is, the Memory box). We also see a typical distributed-memory architecture. Each CPU has access to its own private memory and cannot see any other CPU memory space.\n",
    "\n",
    "<img src=\"images/MemoryOrganization.png\" alt=\"ParallelisedPeaSoup\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 ms, sys: 8.18 ms, total: 26.4 ms\n",
      "Wall time: 23.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49999995000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# wall time: The wall time means that a clock hanging on a wall outside of the computer would measure xxx ms from the time the code was submitted to the CPU to the time when the process completed \n",
    "# CPU time:  The time actually spent by CPU executing  code\n",
    "# User time and sys time both refer to time taken by the CPU to actually work on the code. \n",
    "# CPU time:  The CPU time dedicated to our code is only a fraction of the wall time as the CPU swaps its attention from our code to other processes that are running on the system.\n",
    "# User time: The amount of CPU time taken outside of the kernel.i.e. in the user mode \n",
    "# Sys time:  The amount of CPU time spent in the kernel. \n",
    "# Total time:The total CPU time is user time + sys time.\n",
    "\n",
    "import numpy as np\n",
    "np.arange(10**7).sum() # creates an instance of list with evenly spaced values and sum the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same summation, but using dask to parallelize the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49999995000000\n",
      "CPU times: user 22.3 ms, sys: 2.81 ms, total: 25.1 ms\n",
      "Wall time: 22.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The same summation, but using dask to parallelize the code.\n",
    "# NB: the API for dask arrays mimics that of numpy\n",
    "import dask.array as da\n",
    "work = da.arange(10**7).sum()\n",
    "result = work.compute()\n",
    "\n",
    "print (result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "Apache Spark is a lightning fast real-time processing framework. \n",
    "It does in-memory computations to analyze data in real-time.\n",
    "It came into picture as Apache Hadoop MapReduce was performing batch processing only and lacked a real-time processing feature. \n",
    "Hence, Apache Spark was introduced as it can perform stream processing in real-time and can also take care of batch processing.\n",
    "Apart from real-time and batch processing, Apache Spark supports interactive queries and iterative algorithms also. Apache Spark has its own cluster manager, where it can host its application. It leverages Apache Hadoop for both storage and processing. It uses HDFS (Hadoop Distributed File system) for storage and it can run Spark applications on YARN as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each successive loop uses the result of the previous loop. In that way, it is dependent on the previous loop. The following dependency diagram makes that clear:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark\n",
    "\n",
    "Apache Spark is written in Scala programming language. \n",
    "To support Python with Spark, Apache Spark Community released a tool, PySpark. \n",
    "Using PySpark, you can work with RDDs in Python programming language also. \n",
    "It is because of a library called Py4j that they are able to achieve this.\n",
    "PySpark offers PySpark Shell which links the Python API to the spark core and initializes the Spark context.\n",
    "Majority of data scientists and analytics experts today use Python because of its rich library set. \n",
    "Integrating Python with Spark is a boon to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark shell\n",
    "\n",
    "PySpark shell is referred as REPL (Read Eval Print Loop) which is used to quickly test PySpark statements. \n",
    "Spark shell is available for Scala, Python and R. \n",
    "The pyspark command is used to launch Spark with Python shell also call PySpark.\n",
    "\n",
    "<img src=\"images/pyspark shell.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark in Python - Spark Context\n",
    "\n",
    "The first step in using Spark is connecting to a Spark cluster.\n",
    "A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. \n",
    "SparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. \n",
    "The driver program then runs the operations inside the executors on worker nodes.\n",
    "In practice, the cluster will be hosted on a remote machine that's connected to all other nodes. \n",
    "There will be one computer, called the master that manages splitting up the data and the computations. \n",
    "The master is connected to the rest of the computers in the cluster, which are called worker. \n",
    "The master sends the workers data and calculations to run, and they send their results back to the master.\n",
    "\n",
    "<img src=\"images/sparkContext.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext \n\u001b[1;32m      4\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(SparkConf()\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntro pyspark\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf \n",
    "from pyspark.context import SparkContext \n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\").setAppName(\"Intro pyspark\"))\n",
    "\n",
    "\n",
    "# Verify SparkContext\n",
    "print(sc)\n",
    "\n",
    "# Print Spark version\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPO1Js+wB+gf1zJ0Yn405Oz",
   "mount_file_id": "12ufX2tXkBxOM9c0k9m3sSpcS4BEMPh1H",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
