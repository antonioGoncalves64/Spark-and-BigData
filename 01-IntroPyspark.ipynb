{"metadata":{"colab":{"authorship_tag":"ABX9TyPO1Js+wB+gf1zJ0Yn405Oz","mount_file_id":"12ufX2tXkBxOM9c0k9m3sSpcS4BEMPh1H","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"images/spark.png\" alt=\"drawing\" width=\"200\"/>\n\n# Introduction Spark and Pyspark\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Lambda Function in Lambda Function","metadata":{}},{"cell_type":"markdown","source":"### Introduction\n\nTaken literally, an anonymous function is a function without a name. In Python, an anonymous function is created with the lambda keyword. More loosely, it may or not be assigned a name. \nLambda functions can have any number of arguments but only one expression. The expression is evaluated and returned. Lambda functions can be used wherever function objects are required.\n\n**Syntax: lambda arguments: expression**\n\n1. This function can have any number of arguments but only one expression, which is evaluated and returned.\n1. One is free to use lambda functions wherever function objects are required.\n1. You need to keep in your knowledge that lambda functions are syntactically restricted to a single expression.\n1. It has various uses in particular fields of programming, besides other types of expressions in functions.\n\n","metadata":{}},{"cell_type":"markdown","source":"Consider a two-argument anonymous function defined with lambda but not bound to a variable. The lambda is not given a name.\nThe lambda is not given a name:","metadata":{}},{"cell_type":"code","source":"(lambda x, y: x + y)(2, 3)\n","metadata":{},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}]},{"cell_type":"markdown","source":"Consider a one-argument anonymous function defined with lambda but  bound to a variable","metadata":{}},{"cell_type":"code","source":"# Multiply an element by 2 \ndouble = lambda x: x * 2\n\nprint(double(5))","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","text":"10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"is nearly the same as:","metadata":{}},{"cell_type":"code","source":"def double(x):\n   return x * 2\n\ndouble(5)","metadata":{},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"markdown","source":"In Python, we generally use it as an argument to a higher-order function (a function that takes in other functions as arguments). Lambda functions are used along with built-in functions like filter(), map() etc.","metadata":{}},{"cell_type":"markdown","source":"### Example Using filter function\n\nThe filter() function in Python takes in a function and a list as arguments.\nThe function is called with all the items in the list and a new list is returned which contains items for which the function evaluates to True.","metadata":{}},{"cell_type":"code","source":"# Program to filter out only the items > than 6  from a list\nmy_list = [1, 5, 4, 6, 8, 11, 3, 12]\nprint(my_list)\n\nnew_list = list(filter(lambda x: (x > 6) , my_list))\n\nprint(new_list)","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","text":"[1, 5, 4, 6, 8, 11, 3, 12]\n[8, 11, 12]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Example Using map function\n\nThe map() function in Python takes in a function and a list.\nThe function is called with all the items in the list and a new list is returned which contains items returned by that function for each item.","metadata":{}},{"cell_type":"code","source":"# Program to double each item in a list using map()\n\nmy_list = [1, 5, 4, 6, 8, 11, 3, 12]\n\nnew_list = list(map(lambda x: x * 2 , my_list))\n\nprint(new_list)","metadata":{},"execution_count":9,"outputs":[{"name":"stdout","text":"[2, 10, 8, 12, 16, 22, 6, 24]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Example Using reduce function\n\nThe reduce() function in Python is a function that implements a mathematical technique called folding or reduction. \nreduce() is useful when you need to apply a function to an iterable and reduce it to a single cumulative value.","metadata":{}},{"cell_type":"code","source":"from functools import reduce\n\nlst = [1, 2, 3, 4, 5]\nreduce(lambda x, y: x + y, lst)","metadata":{},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"15"},"metadata":{}}]},{"cell_type":"markdown","source":"### Pros and Cons of a Lambda Function in Python\n\n* Pros\n  * It’s an ideal choice for evaluating a single expression that is supposed to be evaluated only once.\n  * It can be invoked as soon as it is defined.\n  * Its syntax is more compact in comparison to a corresponding normal function.\n  * It can be passed as a parameter to a higher-order function, like filter(), map(), and reduce().\n\n* Cons\n  * It can’t perform multiple expressions.\n  * It can easily become cumbersome, for example when it includes an if-elif-…-else cycle.\n  * It can’t contain any variable assignements (e.g., lambda x: x=0 will throw a SyntaxError).\n  * We can’t provide a docstring to a lambda function.","metadata":{}},{"cell_type":"markdown","source":"## Parallel Programming in Python","metadata":{}},{"cell_type":"markdown","source":"### Parallelizable and non-parallelizable tasks","metadata":{}},{"cell_type":"code","source":"Some tasks are easily parallelizable while others inherently aren’t. \nHowever, it might not always be immediately apparent that a task is parallelizable.\nLet us consider the following piece of code.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = [1, 2, 3, 4] # input\n\nsum = 0 # Initialize output\n\nfor e in x:\n  sum = sum + e # Add each element to the output variable\n\nprint(\"Sum:\", sum) # Print output","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Sum: 10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<img src=\"images/serial.png\" alt=\"serial\" width=\"900\"/>","metadata":{}},{"cell_type":"markdown","source":"Although we are performing the loops in a serial way in the code above, nothing avoids us from performing this calculation in parallel. \nThe following example shows that parts of the computations can be done independently:","metadata":{}},{"cell_type":"code","source":"from functools import reduce\n\n\n\nx = [1, 2, 3, 4] #input\n\nchunk1 = x[:2]\nchunk2 = x[2:]\n\nsum_1 = reduce(lambda x, y: x + y, chunk1)\nsum_2 = reduce(lambda x, y: x + y, chunk2)\n\nresult = sum_1 + sum_2\n\nprint(result)","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<img src=\"images/serial2.png\" alt=\"serial\" width=\"900\"/>\n\nThe technique for parallelising sums like this is called chunking. \n\nThere is a subclass of algorithms where the subtasks are completely independent. \nThese kinds of algorithms are known as embarrassingly parallel, or  parallel algorithms. An example of this kind of problem is squaring each element in a list, which can be done like so:","metadata":{}},{"cell_type":"code","source":"x = [1, 2, 3, 4] #input\n\nprint(\"in: \", x)\n\ny = [n**2 for n in x]\n\nprint(\"out:\", y)","metadata":{"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"in:  [1, 2, 3, 4]\nout: [1, 4, 9, 16]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Each task of squaring a number is independent of all the other elements in the list. It is important to know that some tasks are fundamentally non-parallelizable. An example of such an inherently serial algorithm could be the computation of the fibonacci sequence using the formula Fn=Fn-1 + Fn-2.Each output here depends on the outputs of the two previous loops.","metadata":{}},{"cell_type":"markdown","source":"### Challenge: Parallelised Pea Soup\n\n**We have the following recipe:**\n\n* (01 min) Pour water into a soup pan, add the split peas and bay leaf and bring it to boil.\n* (60 min) Remove any foam using a skimmer and let it simmer under a lid for about 60 minutes.\n* (15 min) Clean and chop the leek, celeriac, onion, carrot and potato.\n* (20 min) Remove the bay leaf, add the vegetables and simmer for 20 more minutes. Stir the soup occasionally.\n* (1 day ) Leave the soup for one day. Reheat before serving and add a sliced smoked sausage (vegetarian options are also welcome). Season with pepper and salt.\n\n**Imagine you’re cooking alone.**\n\n1. Can you identify potential for parallelisation in this recipe?\n1. And what if you are cooking with the help of a friend help? Is the soup done any faster?\n1. Draw a dependency diagram.","metadata":{}},{"cell_type":"markdown","source":"**Solution:**\n    \n1. You can cut vegetables while simmering the split peas.\n1. If you have help, you can parallelize cutting vegetables further.\n1. There are two ‘workers’: the cook and the stove.\n\n\n<img src=\"images/ParallelisedPeaSoup.png\" alt=\"ParallelisedPeaSoup\" width=\"900\"/>","metadata":{}},{"cell_type":"markdown","source":"## Spark\nApache Spark is a lightning fast real-time processing framework. \nIt does in-memory computations to analyze data in real-time.\nIt came into picture as Apache Hadoop MapReduce was performing batch processing only and lacked a real-time processing feature. \nHence, Apache Spark was introduced as it can perform stream processing in real-time and can also take care of batch processing.\nApart from real-time and batch processing, Apache Spark supports interactive queries and iterative algorithms also. Apache Spark has its own cluster manager, where it can host its application. It leverages Apache Hadoop for both storage and processing. It uses HDFS (Hadoop Distributed File system) for storage and it can run Spark applications on YARN as well.","metadata":{}},{"cell_type":"markdown","source":"Each successive loop uses the result of the previous loop. In that way, it is dependent on the previous loop. The following dependency diagram makes that clear:","metadata":{}},{"cell_type":"markdown","source":"## PySpark\n\nApache Spark is written in Scala programming language. \nTo support Python with Spark, Apache Spark Community released a tool, PySpark. \nUsing PySpark, you can work with RDDs in Python programming language also. \nIt is because of a library called Py4j that they are able to achieve this.\nPySpark offers PySpark Shell which links the Python API to the spark core and initializes the Spark context.\nMajority of data scientists and analytics experts today use Python because of its rich library set. \nIntegrating Python with Spark is a boon to them.","metadata":{}},{"cell_type":"markdown","source":"### PySpark shell\n\nPySpark shell is referred as REPL (Read Eval Print Loop) which is used to quickly test PySpark statements. \nSpark shell is available for Scala, Python and R. \nThe pyspark command is used to launch Spark with Python shell also call PySpark.\n\n<img src=\"images/pyspark shell.png\" alt=\"drawing\" width=\"800\"/>\n\n","metadata":{}},{"cell_type":"markdown","source":"### Using Spark in Python - Spark Context\n\nThe first step in using Spark is connecting to a Spark cluster.\nA SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. \nSparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. \nThe driver program then runs the operations inside the executors on worker nodes.\nIn practice, the cluster will be hosted on a remote machine that's connected to all other nodes. \nThere will be one computer, called the master that manages splitting up the data and the computations. \nThe master is connected to the rest of the computers in the cluster, which are called worker. \nThe master sends the workers data and calculations to run, and they send their results back to the master.\n\n<img src=\"images/sparkContext.png\" alt=\"drawing\" width=\"600\"/>","metadata":{}},{"cell_type":"code","source":"from pyspark import SparkConf \nfrom pyspark.context import SparkContext \n\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\").setAppName(\"Intro pyspark\"))\n\n\n# Verify SparkContext\nprint(sc)\n\n# Print Spark version\nprint(sc.version)","metadata":{},"execution_count":1,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"22/11/06 13:32:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n<SparkContext master=local[*] appName=Intro pyspark>\n3.3.0\n","output_type":"stream"}]}]}