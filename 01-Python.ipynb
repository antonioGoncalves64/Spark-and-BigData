{"metadata":{"colab":{"authorship_tag":"ABX9TyPO1Js+wB+gf1zJ0Yn405Oz","mount_file_id":"12ufX2tXkBxOM9c0k9m3sSpcS4BEMPh1H","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"images/spark.png\" alt=\"drawing\" width=\"200\"/>\n\n# Review Python\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Lambda Function in Lambda Function","metadata":{}},{"cell_type":"markdown","source":"### Introduction\n\nTaken literally, an anonymous function is a function without a name. In Python, an anonymous function is created with the lambda keyword. More loosely, it may or not be assigned a name. \nLambda functions can have any number of arguments but only one expression. The expression is evaluated and returned. Lambda functions can be used wherever function objects are required.\n\n**Syntax: lambda arguments: expression**\n\n1. This function can have any number of arguments but only one expression, which is evaluated and returned.\n1. One is free to use lambda functions wherever function objects are required.\n1. You need to keep in your knowledge that lambda functions are syntactically restricted to a single expression.\n1. It has various uses in particular fields of programming, besides other types of expressions in functions.\n\n","metadata":{}},{"cell_type":"markdown","source":"Consider a two-argument anonymous function defined with lambda but not bound to a variable. The lambda is not given a name.\nThe lambda is not given a name:","metadata":{}},{"cell_type":"code","source":"(lambda x, y: x + y)(2, 3)\n","metadata":{},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}]},{"cell_type":"markdown","source":"Consider a one-argument anonymous function defined with lambda but  bound to a variable","metadata":{}},{"cell_type":"code","source":"# Multiply an element by 2 \ndouble = lambda x: x * 2\n\nprint(double(5))","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","text":"10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"is nearly the same as:","metadata":{}},{"cell_type":"code","source":"def double(x):\n   return x * 2\n\ndouble(5)","metadata":{},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"markdown","source":"In Python, we generally use it as an argument to a higher-order function (a function that takes in other functions as arguments). Lambda functions are used along with built-in functions like filter(), map() etc.","metadata":{}},{"cell_type":"markdown","source":"### Example Using filter function\n\nThe filter() function in Python takes in a function and a list as arguments.\nThe function is called with all the items in the list and a new list is returned which contains items for which the function evaluates to True.","metadata":{}},{"cell_type":"code","source":"# Program to filter out only the items > than 6  from a list\nmy_list = [1, 5, 4, 6, 8, 11, 3, 12]\nprint(my_list)\n\nnew_list = list(filter(lambda x: (x > 6) , my_list))\n\nprint(new_list)","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","text":"[1, 5, 4, 6, 8, 11, 3, 12]\n[8, 11, 12]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Example Using map function\n\nThe map() function in Python takes in a function and a list.\nThe function is called with all the items in the list and a new list is returned which contains items returned by that function for each item.","metadata":{}},{"cell_type":"code","source":"# Program to double each item in a list using map()\n\nmy_list = [1, 5, 4, 6, 8, 11, 3, 12]\n\nnew_list = list(map(lambda x: x * 2 , my_list))\n\nprint(new_list)","metadata":{},"execution_count":9,"outputs":[{"name":"stdout","text":"[2, 10, 8, 12, 16, 22, 6, 24]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Example Using reduce function\n\nThe reduce() function in Python is a function that implements a mathematical technique called folding or reduction. \nreduce() is useful when you need to apply a function to an iterable and reduce it to a single cumulative value.","metadata":{}},{"cell_type":"code","source":"from functools import reduce\n\nlst = [1, 2, 3, 4, 5]\nreduce(lambda x, y: x + y, lst)","metadata":{},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"15"},"metadata":{}}]},{"cell_type":"markdown","source":"### Pros and Cons of a Lambda Function in Python\n\n* Pros\n  * It’s an ideal choice for evaluating a single expression that is supposed to be evaluated only once.\n  * It can be invoked as soon as it is defined.\n  * Its syntax is more compact in comparison to a corresponding normal function.\n  * It can be passed as a parameter to a higher-order function, like filter(), map(), and reduce().\n\n* Cons\n  * It can’t perform multiple expressions.\n  * It can easily become cumbersome, for example when it includes an if-elif-…-else cycle.\n  * It can’t contain any variable assignements (e.g., lambda x: x=0 will throw a SyntaxError).\n  * We can’t provide a docstring to a lambda function.","metadata":{}},{"cell_type":"markdown","source":"## Parallel Programming in Python","metadata":{}},{"cell_type":"markdown","source":"### Parallelizable and non-parallelizable tasks","metadata":{}},{"cell_type":"code","source":"Some tasks are easily parallelizable while others inherently aren’t. \nHowever, it might not always be immediately apparent that a task is parallelizable.\nLet us consider the following piece of code.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = [1, 2, 3, 4] # input\n\nsum = 0 # Initialize output\n\nfor e in x:\n  sum = sum + e # Add each element to the output variable\n\nprint(\"Sum:\", sum) # Print output","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","text":"Sum: 10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<img src=\"images/serial.png\" alt=\"serial\" width=\"900\"/>","metadata":{}},{"cell_type":"markdown","source":"Although we are performing the loops in a serial way in the code above, nothing avoids us from performing this calculation in parallel. \nThe following example shows that parts of the computations can be done independently:","metadata":{}},{"cell_type":"code","source":"from functools import reduce\n\n\n\nx = [1, 2, 3, 4] #input\n\nchunk1 = x[:2]\nchunk2 = x[2:]\n\nsum_1 = reduce(lambda x, y: x + y, chunk1)\nsum_2 = reduce(lambda x, y: x + y, chunk2)\n\nresult = sum_1 + sum_2\n\nprint(result)","metadata":{},"execution_count":2,"outputs":[{"name":"stdout","text":"10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<img src=\"images/serial2.png\" alt=\"serial\" width=\"900\"/>\n\nThe technique for parallelising sums like this is called chunking. \n\nThere is a subclass of algorithms where the subtasks are completely independent. \nThese kinds of algorithms are known as embarrassingly parallel, or  parallel algorithms. An example of this kind of problem is squaring each element in a list, which can be done like so:","metadata":{}},{"cell_type":"code","source":"x = [1, 2, 3, 4] #input\n\nprint(\"in: \", x)\n\ny = [n**2 for n in x]\n\nprint(\"out:\", y)","metadata":{},"execution_count":5,"outputs":[{"name":"stdout","text":"in:  [1, 2, 3, 4]\nout: [1, 4, 9, 16]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Each task of squaring a number is independent of all the other elements in the list. It is important to know that some tasks are fundamentally non-parallelizable. An example of such an inherently serial algorithm could be the computation of the fibonacci sequence using the formula Fn=Fn-1 + Fn-2.Each output here depends on the outputs of the two previous loops.","metadata":{}},{"cell_type":"markdown","source":"### Challenge: Parallelised Pea Soup\n\n**We have the following recipe:**\n\n* (01 min) Pour water into a soup pan, add the split peas and bay leaf and bring it to boil.\n* (60 min) Remove any foam using a skimmer and let it simmer under a lid for about 60 minutes.\n* (15 min) Clean and chop the leek, celeriac, onion, carrot and potato.\n* (20 min) Remove the bay leaf, add the vegetables and simmer for 20 more minutes. Stir the soup occasionally.\n* (1 day ) Leave the soup for one day. Reheat before serving and add a sliced smoked sausage (vegetarian options are also welcome). Season with pepper and salt.\n\n**Imagine you’re cooking alone.**\n\n1. Can you identify potential for parallelisation in this recipe?\n1. And what if you are cooking with the help of a friend help? Is the soup done any faster?\n1. Draw a dependency diagram.","metadata":{}},{"cell_type":"markdown","source":"**Solution:**\n    \n1. You can cut vegetables while simmering the split peas.\n1. If you have help, you can parallelize cutting vegetables further.\n1. There are two ‘workers’: the cook and the stove.\n\n\n<img src=\"images/ParallelisedPeaSoup.png\" alt=\"ParallelisedPeaSoup\" width=\"700\"/>","metadata":{}},{"cell_type":"markdown","source":"###  Shared vs. Distributed memory\n\nWhen a program runs on multiple cores with access to the same physical memory, it can use a shared memory design. In shared memory programming, the computation occurs within a single multithreaded process, and each thread within the process has access to memory in the same virtual address space. Communication among threads is efficient because any changes to shared memory are immediately visible to all other threads in that process.\n\nIn contrast, when a program uses multiple processes, such that computational tasks do not share memory, its processes cannot communicate by writing updated values directly to memory. This situation requires a distributed memory design. In distributed memory programming, each process owns part of the data, and other processes must send a message to the owner in order to update that part of the data. The communication could be between processes running on the same node, or between processes running on different nodes in a cluster, but the underlying communication model is the same. Interface standards like Message-Passing Interface (MPI) facilitate distributed memory programming for cluster systems.\n\nIn the following figure, we see a typical shared-memory architecture where four processors (the four CPU boxes in the following diagram) can all access the same memory address space (that is, the Memory box). We also see a typical distributed-memory architecture. Each CPU has access to its own private memory and cannot see any other CPU memory space.\n\n<img src=\"images/MemoryOrganization.png\" alt=\"ParallelisedPeaSoup\" width=\"600\"/>","metadata":{}},{"cell_type":"code","source":"%%time\n\n# wall time: The wall time means that a clock hanging on a wall outside of the computer would measure xxx ms from the time the code was submitted to the CPU to the time when the process completed \n# CPU time:  The time actually spent by CPU executing  code\n# User time and sys time both refer to time taken by the CPU to actually work on the code. \n# CPU time:  The CPU time dedicated to our code is only a fraction of the wall time as the CPU swaps its attention from our code to other processes that are running on the system.\n# User time: The amount of CPU time taken outside of the kernel.i.e. in the user mode \n# Sys time:  The amount of CPU time spent in the kernel. \n# Total time:The total CPU time is user time + sys time.\n\nimport numpy as np\nnp.arange(10**7).sum() # creates an instance of list with evenly spaced values and sum the values","metadata":{},"execution_count":5,"outputs":[{"name":"stdout","text":"CPU times: user 36.6 ms, sys: 197 ms, total: 233 ms\nWall time: 236 ms\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"49999995000000"},"metadata":{}}]},{"cell_type":"markdown","source":"If run the chunk several times, we will notice a difference in the times. How can we trust this timer, then? A possible solution will be to time the chunk several times, and take the average time as our valid measure. Or we can use magic command %%timeit (it does exactly this in a concise an comfortable manner!).","metadata":{}},{"cell_type":"code","source":"%%timeit\n\nimport numpy as np\nnp.arange(10**7).sum() # creates an instance of list with evenly spaced values and sum the values","metadata":{},"execution_count":4,"outputs":[{"name":"stdout","text":"60.4 ms ± 23.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The same operation, but using dask to parallelize the code.","metadata":{}},{"cell_type":"code","source":"%%time\n# The same summation, but using dask to parallelize the code.\n# NB: the API for dask arrays mimics that of numpy\nimport dask.array as da\nwork = da.arange(10**7).sum()\nresult = work.compute()\n\n","metadata":{},"execution_count":3,"outputs":[{"name":"stdout","text":"CPU times: user 223 ms, sys: 81.8 ms, total: 305 ms\nWall time: 310 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"import psutil\nN_physical_cores = psutil.cpu_count(logical=False)\nN_logical_cores = psutil.cpu_count(logical=True)\nprint(f\"The number of physical/logical cores is {N_physical_cores}/{N_logical_cores}\")","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","text":"The number of physical/logical cores is 4/8\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Each successive loop uses the result of the previous loop. In that way, it is dependent on the previous loop. The following dependency diagram makes that clear:","metadata":{}}]}