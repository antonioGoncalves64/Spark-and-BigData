{"metadata":{"colab":{"authorship_tag":"ABX9TyPO1Js+wB+gf1zJ0Yn405Oz","mount_file_id":"12ufX2tXkBxOM9c0k9m3sSpcS4BEMPh1H","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"images/spark.png\" alt=\"drawing\" width=\"200\"/>\n\n# Introduction Spark DataFrame\n\nA DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects. Apache Spark DataFrames provide a rich set of functions (select columns, filter, join, aggregate) that allow you to solve common data analysis problems efficiently.\n\nApache Spark DataFrames are an abstraction built on top of Resilient Distributed Datasets (RDDs). Spark DataFrames and Spark SQL use a unified planning and optimization engine, allowing you to get nearly identical performance across all supported languages (Python, SQL, Scala, and R).\n\n\n","metadata":{}},{"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession\n\n# create a spark session\nspark = SparkSession.builder.master(\"local[1]\").appName('SparkDataFrame').getOrCreate()\n\n# get a spark contex\nsc = spark.sparkContext\n\n\nprint(\"APP Name :\"+sc.appName);\nprint(\"Master   :\"+ sc.master);\n","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"22/11/10 21:49:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nAPP Name :SparkDataFrame\nMaster   :local[1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Create a DataFrame from RDD\n\nMost Apache Spark queries return a DataFrame. This includes reading from a table, loading data from files, and operations that transform data.\n\n","metadata":{}},{"cell_type":"code","source":"\n\niphones_RDD = sc.parallelize([ (\"XS\", 2018, 5.65, 2.79, 6.24), \\\n(\"XR\", 2018, 5.94, 2.98, 6.84),\\\n(\"X10\", 2017, 5.65, 2.79, 6.13),\\\n(\"8Plus\", 2017, 6.23, 3.07, 7.12)\\\n])\n\nnames = ['Model', 'Year', 'Height', 'Width', 'Weight']\n\niphones_df = spark.createDataFrame(iphones_RDD, schema=names)\niphones_df.show()\n\n\n","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+-----+----+------+-----+------+\n|Model|Year|Height|Width|Weight|\n+-----+----+------+-----+------+\n|   XS|2018|  5.65| 2.79|  6.24|\n|   XR|2018|  5.94| 2.98|  6.84|\n|  X10|2017|  5.65| 2.79|  6.13|\n|8Plus|2017|  6.23| 3.07|  7.12|\n+-----+----+------+-----+------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Create a DataFrame from pandas\n\nYou can also create a Spark DataFrame from a list or a pandas DataFrame, such as in the following example:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndata = [[1, \"Elia\"], [2, \"Teo\"], [3, \"Fang\"]]\n\npdf = pd.DataFrame(data, columns=[\"i\", \"n\"])\n\ndf1 = spark.createDataFrame(pdf)\ndf1.show()\n\ndf2 = spark.createDataFrame(data, schema=\"id LONG, name STRING\")\ndf2.show()","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"+---+----+\n|  i|   n|\n+---+----+\n|  1|Elia|\n|  2| Teo|\n|  3|Fang|\n+---+----+\n\n+---+----+\n| id|name|\n+---+----+\n|  1|Elia|\n|  2| Teo|\n|  3|Fang|\n+---+----+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load data into a DataFrame from files\n\nYou can load data from many supported file formats. The following example uses a dataset available in the datasets directory.","metadata":{}},{"cell_type":"code","source":"# CSV format\ndf_csv = spark.read.csv(\"files/ratings.csv\", header=True, inferSchema=True)\n\ndf_csv.tail(3)","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"[Row(userId=5987, movieId=70, rating=5.0, timestamp=1382563110),\n Row(userId=5987, movieId=150, rating=2.5, timestamp=1382564911),\n Row(userId=5987, movieId=152, rating=5.0, timestamp=1382756733)]"},"metadata":{}}]},{"cell_type":"markdown","source":"Load data from a json file","metadata":{}},{"cell_type":"code","source":"\nratingDF = spark.read.json(\"files/ratings.json\")\n\n# The inferred schema can be visualized using the printSchema() method\nratingDF.printSchema()\n\nratingDF.show(5)","metadata":{"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"root\n |-- movieId: long (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: long (nullable = true)\n |-- userId: long (nullable = true)\n\n+-------+------+----------+------+\n|movieId|rating| timestamp|userId|\n+-------+------+----------+------+\n|    296|   5.0|1147880044|     1|\n|    306|   3.5|1147868817|     1|\n|    307|   5.0|1147868828|     1|\n|    665|   5.0|1147878820|     1|\n|    899|   3.5|1147868510|     1|\n+-------+------+----------+------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":" ## DataFrame Operations\n \n Like RDD Spark suporte  support two types of Data Frame operations: transformations and actions\n \n* Transformations  create a new dataset from an existing one.\n* Actions, which return a value to the driver program after running a computation on the dataset. \n\nFor example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n\nAll transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n\n<img src=\"images/lazyTranformation.png\" alt=\"drawing\" width=\"900\"/>\n\nHere, first an RDD is calculated by reading data from a stable storage and two of the transformations are performed on the RDD and then finally an action is performed to get the result.\n\nBy default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.","metadata":{"id":"NkyAuPgERtXS"}},{"cell_type":"markdown","source":" ### Transformations","metadata":{}},{"cell_type":"markdown","source":"#### select( ) - Transformation subsets the columns in the DataFrame\n","metadata":{}},{"cell_type":"code","source":"ratingDF = spark.read.json(\"files/ratings.json\")\n\nrating_value_df = ratingDF.select(\"rating\")\n\nrating_value_df.show(5)","metadata":{"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+------+\n|rating|\n+------+\n|   5.0|\n|   3.5|\n|   5.0|\n|   5.0|\n|   3.5|\n+------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### filter( ) transformation filters out the rows based on a condition","metadata":{}},{"cell_type":"code","source":"ratingDF = spark.read.json(\"files/ratings.json\")\n\nnew_df_rating_3 = ratingDF.filter(ratingDF.rating > 3.0)\n\nnew_df_rating_3.show(5)","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+-------+------+----------+------+\n|movieId|rating| timestamp|userId|\n+-------+------+----------+------+\n|    296|   5.0|1147880044|     1|\n|    306|   3.5|1147868817|     1|\n|    307|   5.0|1147868828|     1|\n|    665|   5.0|1147878820|     1|\n|    899|   3.5|1147868510|     1|\n+-------+------+----------+------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### groupby() can be used to group a variable","metadata":{"tags":[]}},{"cell_type":"code","source":"ratingDF = spark.read.json(\"files/ratings.json\")\n\nnew_df_groupByRating = ratingDF.groupby('rating')\nnew_df_groupByRating.count().show()","metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"collapsed":true,"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"[Stage 18:>                                                         (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+------+------+\n|rating| count|\n+------+------+\n|   3.5|110991|\n|   4.5| 75423|\n|   2.5| 44690|\n|   1.0| 26796|\n|   4.0|237958|\n|   0.5| 12396|\n|   3.0|176963|\n|   2.0| 59568|\n|   1.5| 14815|\n|   5.0|127548|\n+------+------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"RDD = sc.parallelize([\"hello word\", \"How are you\"])\nRDD_flatMap = RDD.flatMap(lambda x : x.split(\" \"))\nprint (\"RDD_flatMap: \", RDD_flatMap.collect()) # action convert to a  List","metadata":{"jupyter":{"source_hidden":true},"tags":[]},"execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":"RDD_flatMap:  ['hello', 'word', 'How', 'are', 'you']\n"}]},{"cell_type":"markdown","source":"#### orderBy() operation sorts the DataFrame based on one or more columns","metadata":{}},{"cell_type":"code","source":"ratingDF = spark.read.json(\"files/ratings.json\")\n\nnew_df_groupByRating = ratingDF.groupby('rating')\nnew_df_groupByRating.count().orderBy('rating').show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[],"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"[Stage 23:>                                                         (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+------+------+\n|rating| count|\n+------+------+\n|   0.5| 12396|\n|   1.0| 26796|\n|   1.5| 14815|\n|   2.0| 59568|\n|   2.5| 44690|\n|   3.0|176963|\n|   3.5|110991|\n|   4.0|237958|\n|   4.5| 75423|\n|   5.0|127548|\n+------+------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"#### dropDuplicates() removes the duplicate rows of a DataFrame","metadata":{}},{"cell_type":"code","source":"ratingDF = spark.read.json(\"files/ratings.json\")\n\nnewDf = ratingDF.select('userId', 'rating').dropDuplicates()\nnewDf.count()\n\n\n\n","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"39130"},"metadata":{}}]},{"cell_type":"markdown","source":"#### withColumnRenamed() renames a column in the DataFrame","metadata":{}},{"cell_type":"code","source":"ratingDF = spark.read.json(\"files/ratings.json\")\n\nnewDf = ratingDF.withColumnRenamed('userId','user')\nnewDf.show()","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"[Stage 55:>                                                         (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+-------+------+----------+----+\n|movieId|rating| timestamp|user|\n+-------+------+----------+----+\n|    296|   5.0|1147880044|   1|\n|    306|   3.5|1147868817|   1|\n|    307|   5.0|1147868828|   1|\n|    665|   5.0|1147878820|   1|\n|    899|   3.5|1147868510|   1|\n|   1088|   4.0|1147868495|   1|\n|   1175|   3.5|1147868826|   1|\n|   1217|   3.5|1147878326|   1|\n|   1237|   5.0|1147868839|   1|\n|   1250|   4.0|1147868414|   1|\n|   1260|   3.5|1147877857|   1|\n|   1653|   4.0|1147868097|   1|\n|   2011|   2.5|1147868079|   1|\n|   2012|   2.5|1147868068|   1|\n|   2068|   2.5|1147869044|   1|\n|   2161|   3.5|1147868609|   1|\n|   2351|   4.5|1147877957|   1|\n|   2573|   4.0|1147878923|   1|\n|   2632|   5.0|1147878248|   1|\n|   2692|   5.0|1147869100|   1|\n+-------+------+----------+----+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":" ### Actions","metadata":{}},{"cell_type":"markdown","source":"#### printSchema() method prints the types of columns in the DataFrame","metadata":{}},{"cell_type":"code","source":"ratingDF = spark.read.json(\"files/ratings.json\")\nratingDF.printSchema()","metadata":{"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"[Stage 65:>                                                         (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"root\n |-- movieId: long (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: long (nullable = true)\n |-- userId: long (nullable = true)\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"#### columns()  prints the columns of a DataFrame","metadata":{}},{"cell_type":"code","source":"ratingDF = spark.read.json(\"files/ratings.json\")\nratingDF.columns","metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['movieId', 'rating', 'timestamp', 'userId']"},"metadata":{}}]},{"cell_type":"markdown","source":"#### describe() operation compute summary statistics of numerical columns in the DataFrame","metadata":{}},{"cell_type":"code","source":"ratingDF = spark.read.json(\"files/ratings.json\")\nratingDF.describe().show()","metadata":{"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"[Stage 69:>                                                         (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+-------+------------------+------------------+--------------------+------------------+\n|summary|           movieId|            rating|           timestamp|            userId|\n+-------+------------------+------------------+--------------------+------------------+\n|  count|            887148|            887148|              887148|            887148|\n|   mean|21212.241609066357| 3.533131450445698|1.2092062081188483E9|3041.6676157754964|\n| stddev|39197.518981108624|1.0521983681122566| 2.302547044992259E8|1714.4553106441933|\n|    min|                 1|               0.5|           789652009|                 1|\n|    max|            208939|               5.0|          1574288328|              5987|\n+-------+------------------+------------------+--------------------+------------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]}]}