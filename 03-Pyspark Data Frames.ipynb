{"metadata":{"colab":{"authorship_tag":"ABX9TyPO1Js+wB+gf1zJ0Yn405Oz","mount_file_id":"12ufX2tXkBxOM9c0k9m3sSpcS4BEMPh1H","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"images/spark.png\" alt=\"drawing\" width=\"200\"/>\n\n# Introduction Spark DataFrame\n\nA DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects. Apache Spark DataFrames provide a rich set of functions (select columns, filter, join, aggregate) that allow you to solve common data analysis problems efficiently.\n\nApache Spark DataFrames are an abstraction built on top of Resilient Distributed Datasets (RDDs). Spark DataFrames and Spark SQL use a unified planning and optimization engine, allowing you to get nearly identical performance across all supported languages (Python, SQL, Scala, and R).\n\n\n","metadata":{}},{"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession\n\n# create a spark session\nspark = SparkSession.builder.master(\"local[1]\").appName('SparkDataFrame').getOrCreate()\n\n# get a spark contex\nsc = spark.sparkContext\n\n\nprint(\"APP Name :\"+sc.appName);\nprint(\"Master   :\"+ sc.master);\n","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"22/11/10 18:38:33 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\nAPP Name :SparkByExamples.com\nMaster   :local[1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Create a DataFrame from RDD\n\nMost Apache Spark queries return a DataFrame. This includes reading from a table, loading data from files, and operations that transform data.\n\n","metadata":{}},{"cell_type":"code","source":"from pyspark import SparkConf \nfrom pyspark.context import SparkContext \n\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\").setAppName(\"Intro pyspark\"))\n\niphones_RDD = sc.parallelize([ (\"XS\", 2018, 5.65, 2.79, 6.24), \\\n(\"XR\", 2018, 5.94, 2.98, 6.84),\\\n(\"X10\", 2017, 5.65, 2.79, 6.13),\\\n(\"8Plus\", 2017, 6.23, 3.07, 7.12)\\\n])\n\nnames = ['Model', 'Year', 'Height', 'Width', 'Weight']\n\niphones_df = spark.createDataFrame(iphones_RDD, schema=names)\niphones_df.show()\n\n\n","metadata":{},"execution_count":35,"outputs":[{"name":"stdout","output_type":"stream","text":"+-----+----+------+-----+------+\n|Model|Year|Height|Width|Weight|\n+-----+----+------+-----+------+\n|   XS|2018|  5.65| 2.79|  6.24|\n|   XR|2018|  5.94| 2.98|  6.84|\n|  X10|2017|  5.65| 2.79|  6.13|\n|8Plus|2017|  6.23| 3.07|  7.12|\n+-----+----+------+-----+------+\n\n"}]},{"cell_type":"markdown","source":"## Create a DataFrame from pandas\n\nYou can also create a Spark DataFrame from a list or a pandas DataFrame, such as in the following example:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndata = [[1, \"Elia\"], [2, \"Teo\"], [3, \"Fang\"]]\n\npdf = pd.DataFrame(data, columns=[\"i\", \"n\"])\n\ndf1 = spark.createDataFrame(pdf)\ndf1.show()\n\ndf2 = spark.createDataFrame(data, schema=\"id LONG, name STRING\")\ndf2.show()","metadata":{},"execution_count":36,"outputs":[{"name":"stdout","output_type":"stream","text":"+---+----+\n|  i|   n|\n+---+----+\n|  1|Elia|\n|  2| Teo|\n|  3|Fang|\n+---+----+\n\n+---+----+\n| id|name|\n+---+----+\n|  1|Elia|\n|  2| Teo|\n|  3|Fang|\n+---+----+\n\n"}]},{"cell_type":"markdown","source":"## Load data into a DataFrame from files\n\nYou can load data from many supported file formats. The following example uses a dataset available in the datasets directory.","metadata":{}},{"cell_type":"code","source":"# CSV format\ndf_csv = spark.read.csv(\"files/ratings.csv\", header=True, inferSchema=True)\n\ndf_csv.tail(3)","metadata":{},"execution_count":37,"outputs":[{"name":"stderr","output_type":"stream","text":"                                                                                \r"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"[Row(userId=5987, movieId=70, rating=5.0, timestamp=1382563110),\n Row(userId=5987, movieId=150, rating=2.5, timestamp=1382564911),\n Row(userId=5987, movieId=152, rating=5.0, timestamp=1382756733)]"},"metadata":{}}]},{"cell_type":"code","source":"\npeopleDF = spark.read.json(\"files/people.json\")\n\n# The inferred schema can be visualized using the printSchema() method\npeopleDF.printSchema()","metadata":{},"execution_count":39,"outputs":[{"ename":"AnalysisException","evalue":"Path does not exist: file:/home/jovyan/files/people.json","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_61/2226248290.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpeopleDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"files/people.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# The inferred schema can be visualized using the printSchema() method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpeopleDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/jovyan/files/people.json"]}]},{"cell_type":"markdown","source":"## PySpark\n\nApache Spark is written in Scala programming language. \nTo support Python with Spark, Apache Spark Community released a tool, PySpark. \nUsing PySpark, you can work with RDDs in Python programming language also. \nIt is because of a library called Py4j that they are able to achieve this.\nPySpark offers PySpark Shell which links the Python API to the spark core and initializes the Spark context.\nMajority of data scientists and analytics experts today use Python because of its rich library set. \nIntegrating Python with Spark is a boon to them.","metadata":{}},{"cell_type":"markdown","source":"### PySpark shell\n\nPySpark shell is referred as REPL (Read Eval Print Loop) which is used to quickly test PySpark statements. \nSpark shell is available for Scala, Python and R. \nThe pyspark command is used to launch Spark with Python shell also call PySpark.\n\n<img src=\"images/pyspark shell.png\" alt=\"drawing\" width=\"800\"/>\n\n","metadata":{}},{"cell_type":"markdown","source":"### Using Spark in Python - Spark Context\n\nThe first step in using Spark is connecting to a Spark cluster.\nA SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. \nSparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. \nThe driver program then runs the operations inside the executors on worker nodes.\nIn practice, the cluster will be hosted on a remote machine that's connected to all other nodes. \nThere will be one computer, called the master that manages splitting up the data and the computations. \nThe master is connected to the rest of the computers in the cluster, which are called worker. \nThe master sends the workers data and calculations to run, and they send their results back to the master.\n\n<img src=\"images/sparkContext.png\" alt=\"drawing\" width=\"600\"/>","metadata":{}},{"cell_type":"code","source":"from pyspark import SparkConf \nfrom pyspark.context import SparkContext \n\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\").setAppName(\"Intro pyspark\"))\n\n\n# Verify SparkContext\nprint(sc)\n\n# Print Spark version\nprint(sc.version)","metadata":{"scrolled":true},"execution_count":1,"outputs":[{"name":"stderr","output_type":"stream","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"},{"name":"stdout","output_type":"stream","text":"22/11/06 13:32:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n<SparkContext master=local[*] appName=Intro pyspark>\n3.3.0\n"}]},{"cell_type":"markdown","source":"-------------","metadata":{}},{"cell_type":"markdown","source":"# Resilient Distributed Datasets (RDDs)\n\nSpark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. \nThere are two ways to create RDDs: \n\n* parallelizing an existing collection in your driver program, or \n* referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.","metadata":{"id":"Yktgv1AMM7tM"}},{"cell_type":"markdown","source":"## Parallelized collection\n\nParallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create two parallelized collection: one for holding the numbers 1 to 5 and the other for hoding a String.","metadata":{"id":"_57oDHuCNYPO"}},{"cell_type":"code","source":"numRDD = sc.parallelize([1,2,3,4,5])\nprint (\"numRDD:   \", type(numRDD)) #confirm type of object RDD\n\nhelloRDD = sc.parallelize((\"Hello world\"))\nprint (\"helloRDD: \",type(helloRDD)) #confirm type of object RDD","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":264,"status":"ok","timestamp":1667593780330,"user":{"displayName":"Antonio Goncalves","userId":"08143767804389786702"},"user_tz":0},"id":"Fl6IOUZuOde0","outputId":"17986c6b-aff9-446f-a533-0125b1bf4dae"},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"numRDD:    <class 'pyspark.rdd.RDD'>\nhelloRDD:  <class 'pyspark.rdd.RDD'>\n"}]},{"cell_type":"markdown","source":"Once created, the distributed dataset (distData) can be operated on in parallel. \nFor example, we can call distData.reduce(lambda a, b: a + b) to add up the elements of the list. ","metadata":{}},{"cell_type":"code","source":"distData = sc.parallelize([1,2,3])\ndistData.reduce(lambda a, b: a + b)","metadata":{},"execution_count":4,"outputs":[{"name":"stderr","output_type":"stream","text":"                                                                                \r"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}]},{"cell_type":"markdown","source":"One important parameter for parallel collections is the number of partitions to cut the dataset into. \nSpark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. \nNormally, Spark tries to set the number of partitions automatically based on your cluster. \nHowever, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10))","metadata":{}},{"cell_type":"code","source":"data01 = sc.parallelize([1,2,3,4,5])\nprint (\"Data01 NumPartitions: \", data01.getNumPartitions())\n\ndata02 = sc.parallelize([1,2,3,4,5],3)\nprint (\"data02 NumPartitions: \", data02.getNumPartitions())","metadata":{},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"Data01 NumPartitions:  1\ndata02 NumPartitions:  3\n"}]},{"cell_type":"markdown","source":"## External Datasets\n\nPySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n\nText file RDDs can be created using SparkContext’s textFile method. This method takes a URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines. Here is an example invocation:","metadata":{"id":"rEsOKslyQIHp"}},{"cell_type":"code","source":"fileRDD = sc.textFile(\"files/ratings.csv\")\n\nnewRDD= fileRDD.take(3)\n\nfor i in newRDD:\n    print(i)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":346,"status":"ok","timestamp":1667593782730,"user":{"displayName":"Antonio Goncalves","userId":"08143767804389786702"},"user_tz":0},"id":"6QVHkFAcQR95","outputId":"be49aced-f737-47dd-bf83-d06e9f7bdad2"},"execution_count":4,"outputs":[{"name":"stderr","output_type":"stream","text":"[Stage 0:>                                                          (0 + 1) / 1]\r"},{"name":"stdout","output_type":"stream","text":"userId,movieId,rating,timestamp\n1,296,5.0,1147880044\n1,306,3.5,1147868817\n"},{"name":"stderr","output_type":"stream","text":"                                                                                \r"}]},{"cell_type":"markdown","source":" ## RDD Operations\n \n RDDs support two types of operations: transformations and actions\n \n* Transformations  create a new dataset from an existing one.\n* Actions, which return a value to the driver program after running a computation on the dataset. \n\nFor example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n\nAll transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n\n<img src=\"images/lazyTranformation.png\" alt=\"drawing\" width=\"900\"/>\n\nHere, first an RDD is calculated by reading data from a stable storage and two of the transformations are performed on the RDD and then finally an action is performed to get the result.\n\nBy default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.","metadata":{"id":"NkyAuPgERtXS"}},{"cell_type":"markdown","source":" ### Transformations","metadata":{}},{"cell_type":"markdown","source":"#### map( ) - Return a new RDD by applying a function to each element of this RDD.","metadata":{}},{"cell_type":"code","source":"RDD = sc.parallelize([1,2,3,4,5])\nRDD_map = RDD.map(lambda x : x * 2)\nprint (\"RDD_map: \",RDD_map.collect()) # action convert to a  List","metadata":{},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"RDD_map:  [2, 4, 6, 8, 10]\n"}]},{"cell_type":"markdown","source":"#### filter( ) returns a new RDD with only the elements that pass the condition","metadata":{}},{"cell_type":"code","source":"RDD = sc.parallelize([1,2,3,4])\nRDD_filter = RDD.filter(lambda x : x >2)\nprint (\"RDD_filter: \", RDD_filter.collect()) # action convert to a  List","metadata":{},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"RDD_filter:  [3, 4]\n"}]},{"cell_type":"markdown","source":"#### flatMap( ) returns multiple values for each element in the original RDD","metadata":{}},{"cell_type":"code","source":"RDD = sc.parallelize([\"hello word\", \"How are you\"])\nRDD_flatMap = RDD.flatMap(lambda x : x.split(\" \"))\nprint (\"RDD_flatMap: \", RDD_flatMap.collect()) # action convert to a  List","metadata":{},"execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":"RDD_flatMap:  ['hello', 'word', 'How', 'are', 'you']\n"}]},{"cell_type":"markdown","source":"#### union( ) Return the union of this RDD and another one","metadata":{}},{"cell_type":"code","source":"rdd01 = sc.parallelize([1, 3, 5, 7])\nrdd02 = sc.parallelize([2, 4, 6, 8])\nrdd03 = rdd01.union(rdd02)\nrdd03.collect()","metadata":{},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[1, 3, 5, 7, 2, 4, 6, 8]"},"metadata":{}}]},{"cell_type":"markdown","source":"### Actions ","metadata":{"id":"QTTj9P9PpD6b"}},{"cell_type":"markdown","source":"#### collection ( ) Return a list that contains all of the elements in this RDD.\nNote: This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.","metadata":{}},{"cell_type":"code","source":"data = [1,2,3,4,5,6,7,8,9,10,11,12]\nrdd  = sc.parallelize(data)\n\nnewData = rdd.collect()\nfor d in newData:\n    print (f\"Value: {d}\")\n","metadata":{},"execution_count":14,"outputs":[{"name":"stdout","output_type":"stream","text":"Value: 1\nValue: 2\nValue: 3\nValue: 4\nValue: 5\nValue: 6\nValue: 7\nValue: 8\nValue: 9\nValue: 10\nValue: 11\nValue: 12\n"}]},{"cell_type":"markdown","source":"#### take(num) – Take the first num elements of the RDD.","metadata":{}},{"cell_type":"code","source":"data = [1,2,3,4,5,6,7,8,9,10,11,12]\nrdd  = sc.parallelize(data)\n\nnewData = rdd.take(2)\nfor d in newData:\n    print (f\"Value: {d}\")","metadata":{},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"Value: 1\nValue: 2\n"}]},{"cell_type":"markdown","source":"#### first( ) – Returns the first record of the RDD","metadata":{}},{"cell_type":"code","source":"data = [1,2,3,4,5,6,7,8,9,10,11,12]\nrdd  = sc.parallelize(data)\n\nnewData = rdd.first()\nprint (f\"Value: {newData}\")","metadata":{},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"Value: 1\n"}]},{"cell_type":"markdown","source":"#### count( ) – Returns the number of records in an RDD","metadata":{}},{"cell_type":"code","source":"data = [1,2,3,4,5,6,7,8,9,10,11,12]\nrdd  = sc.parallelize(data)\n\nnum = rdd.count()\nprint (f\"Count: {num}\")","metadata":{},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"Count: 12\n"}]},{"cell_type":"markdown","source":"#### max( ) – Returns max record","metadata":{}},{"cell_type":"code","source":"data = [1,2,3,4,5,6,7,8,9,10,11,12]\nrdd  = sc.parallelize(data)\n\nnum = rdd.max()\nprint (f\"Max: {num}\")","metadata":{},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"Max: 12\n"}]},{"cell_type":"markdown","source":"#### reduce( ) – Reduces the records to single, we can use this to count or sum.","metadata":{}},{"cell_type":"code","source":"data = [1,2,3,4,5,6,7,8,9,10,11,12]\nrdd  = sc.parallelize(data)\n\nnum = rdd.reduce(lambda a,b: (a+b))\nprint (f\"Max: {num}\")","metadata":{},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"Max: 78\n"}]},{"cell_type":"markdown","source":"## Pair RDDs\n\nSpark Paired RDDs are RDDs containing a key-value pair. Key-value pair (KVP) consists of a two linked data item in it. Here, the key is the identifier, whereas value is the data corresponding to the key value.","metadata":{}},{"cell_type":"markdown","source":"### Creating Pair RDDs\n\nTwo common ways to create pair RDD:\n * From a list of key-value tuples\n * from a regular RDD","metadata":{}},{"cell_type":"markdown","source":"#### Create a Pair RDD from regular RDD","metadata":{}},{"cell_type":"code","source":"\nrdd = sc.parallelize([\"b\", \"a\", \"c\"])\nsorted(rdd.map(lambda x: (x, 1)).collect())","metadata":{},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[('a', 1), ('b', 1), ('c', 1)]"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Create a Pair RDD from a list","metadata":{}},{"cell_type":"code","source":"rdd = sc.parallelize([(1,\"a\"), (2,\"b\"), (3,\"c\")])\nrdd.collect()","metadata":{},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[(1, 'a'), (2, 'b'), (3, 'c')]"},"metadata":{}}]},{"cell_type":"markdown","source":"### Transformations on pair RDDs\nAll regular transformations work on pair RDD. Have to pass functions that operate on key value pairs rather than on individual elements","metadata":{}},{"cell_type":"markdown","source":"#### reduceByKey(fun) - groups all the values with the same key.","metadata":{}},{"cell_type":"code","source":"rdd = sc.parallelize([(\"a\",1), (\"b\",2), (\"c\", 10),(\"a\", 2), (\"d\", 5), (\"a\", 4) ])\nrdd_reduceByKey = rdd.reduceByKey(lambda x, y: x+y )\nrdd_reduceByKey.collect()","metadata":{},"execution_count":22,"outputs":[{"name":"stderr","output_type":"stream","text":"                                                                                \r"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"[('a', 7), ('b', 2), ('c', 10), ('d', 5)]"},"metadata":{}}]},{"cell_type":"markdown","source":"#### sortByKey(fun) - Order RDD pair by key.","metadata":{}},{"cell_type":"code","source":"rdd = sc.parallelize([(\"a\",1), (\"c\",2), (\"b\", 10),(\"a\", 2), (\"d\", 5), (\"a\", 4) ])\nrdd_reduceByKey = rdd.reduceByKey(lambda x, y: x+y )\nrdd_reduceByKey.sortByKey(ascending = True).collect()","metadata":{},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[('a', 7), ('b', 10), ('c', 2), ('d', 5)]"},"metadata":{}}]},{"cell_type":"markdown","source":"#### groupByKey( ) - Groups all the values with the same key in the pair ","metadata":{}},{"cell_type":"code","source":"rdd = sc.parallelize([(\"a\",1), (\"c\",2), (\"b\", 10),(\"a\", 2), (\"d\", 5), (\"a\", 4) ])\nrdd_groupByKey = rdd.groupByKey().collect()\nfor letter, value in  rdd_groupByKey:\n    print (letter, list(value))","metadata":{},"execution_count":24,"outputs":[{"name":"stdout","output_type":"stream","text":"a [1, 2, 4]\nc [2]\nb [10]\nd [5]\n"}]},{"cell_type":"markdown","source":"#### join( ) - transformation joins the two pair RDDs based on their key","metadata":{}},{"cell_type":"code","source":"rdd01 = sc.parallelize([(\"a\",1), (\"b\", 5),(\"c\", 7) ])\nrdd02 = sc.parallelize([(\"a\",2), (\"b\", 3),(\"d\", 4) ])\n\nrdd01. join(rdd02).collect()\n","metadata":{},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[('b', (5, 3)), ('a', (1, 2))]"},"metadata":{}}]},{"cell_type":"markdown","source":"#### countByKey( ) - action counts the number of elements for each key","metadata":{}},{"cell_type":"code","source":"rdd = sc.parallelize([(\"a\",2), (\"b\", 4),(\"a\", 3) ])\nfor key, val in  rdd.countByKey().items():\n    print (key, val)","metadata":{},"execution_count":26,"outputs":[{"name":"stdout","output_type":"stream","text":"a 2\nb 1\n"}]},{"cell_type":"markdown","source":"#### collectAsMap( ) - action return the key-value pairs in the RDD as a dictionary","metadata":{}},{"cell_type":"code","source":"rdd = sc.parallelize([(\"a\",2), (\"b\", 4),(\"c\", 3) ])\nrdd.collectAsMap()","metadata":{},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'a': 2, 'b': 4, 'c': 3}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}