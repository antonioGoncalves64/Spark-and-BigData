{"metadata":{"colab":{"authorship_tag":"ABX9TyPO1Js+wB+gf1zJ0Yn405Oz","mount_file_id":"12ufX2tXkBxOM9c0k9m3sSpcS4BEMPh1H","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"images/spark.png\" alt=\"drawing\" width=\"200\"/>\n\n# Introduction Spark and Pyspark\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Python ","metadata":{}},{"cell_type":"markdown","source":"### Lambda Function\n\nTaken literally, an anonymous function is a function without a name. In Python, an anonymous function is created with the lambda keyword. More loosely, it may or not be assigned a name. \nLambda functions can have any number of arguments but only one expression. The expression is evaluated and returned. Lambda functions can be used wherever function objects are required.\n\nConsider a two-argument anonymous function defined with lambda but not bound to a variable. The lambda is not given a name.\nThe lambda is not given a name:","metadata":{}},{"cell_type":"code","source":"(lambda x, y: x + y)(2, 3)\n","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}]},{"cell_type":"code","source":"# Program to show the use of lambda functions\ndouble = lambda x: x * 2\n\nprint(double(5))","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"is nearly the same as:","metadata":{}},{"cell_type":"code","source":"def double(x):\n   return x * 2\n\ndouble(5)","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"markdown","source":"In Python, we generally use it as an argument to a higher-order function (a function that takes in other functions as arguments). Lambda functions are used along with built-in functions like filter(), map() etc.","metadata":{}},{"cell_type":"markdown","source":"#### Example Using filter function\n\nThe filter() function in Python takes in a function and a list as arguments.\nThe function is called with all the items in the list and a new list is returned which contains items for which the function evaluates to True.","metadata":{}},{"cell_type":"code","source":"# Program to filter out only the even items from a list\nmy_list = [1, 5, 4, 6, 8, 11, 3, 12]\nprint(my_list)\n\n\nnew_list = list(filter(lambda x: (x > 6) , my_list))\n\nprint(new_list)","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[1, 5, 4, 6, 8, 11, 3, 12]\n[8, 11, 12]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Example Using map function\n\nThe map() function in Python takes in a function and a list.\nThe function is called with all the items in the list and a new list is returned which contains items returned by that function for each item.","metadata":{}},{"cell_type":"code","source":"# Program to double each item in a list using map()\n\nmy_list = [1, 5, 4, 6, 8, 11, 3, 12]\n\nnew_list = list(map(lambda x: x * 2 , my_list))\n\nprint(new_list)","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[2, 10, 8, 12, 16, 22, 6, 24]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Spark\nApache Spark is a lightning fast real-time processing framework. \nIt does in-memory computations to analyze data in real-time.\nIt came into picture as Apache Hadoop MapReduce was performing batch processing only and lacked a real-time processing feature. \nHence, Apache Spark was introduced as it can perform stream processing in real-time and can also take care of batch processing.\nApart from real-time and batch processing, Apache Spark supports interactive queries and iterative algorithms also. Apache Spark has its own cluster manager, where it can host its application. It leverages Apache Hadoop for both storage and processing. It uses HDFS (Hadoop Distributed File system) for storage and it can run Spark applications on YARN as well.","metadata":{}},{"cell_type":"markdown","source":"## PySpark\n\nApache Spark is written in Scala programming language. \nTo support Python with Spark, Apache Spark Community released a tool, PySpark. \nUsing PySpark, you can work with RDDs in Python programming language also. \nIt is because of a library called Py4j that they are able to achieve this.\nPySpark offers PySpark Shell which links the Python API to the spark core and initializes the Spark context.\nMajority of data scientists and analytics experts today use Python because of its rich library set. \nIntegrating Python with Spark is a boon to them.","metadata":{}},{"cell_type":"markdown","source":"### PySpark shell\n\nPySpark shell is referred as REPL (Read Eval Print Loop) which is used to quickly test PySpark statements. \nSpark shell is available for Scala, Python and R. \nThe pyspark command is used to launch Spark with Python shell also call PySpark.\n\n<img src=\"images/pyspark shell.png\" alt=\"drawing\" width=\"800\"/>\n\n","metadata":{}},{"cell_type":"markdown","source":"### Using Spark in Python - Spark Context\n\nThe first step in using Spark is connecting to a Spark cluster.\nA SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. \nSparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. \nThe driver program then runs the operations inside the executors on worker nodes.\nIn practice, the cluster will be hosted on a remote machine that's connected to all other nodes. \nThere will be one computer, called the master that manages splitting up the data and the computations. \nThe master is connected to the rest of the computers in the cluster, which are called worker. \nThe master sends the workers data and calculations to run, and they send their results back to the master.\n\n<img src=\"images/sparkContext.png\" alt=\"drawing\" width=\"600\"/>","metadata":{}},{"cell_type":"code","source":"from pyspark import SparkConf \nfrom pyspark.context import SparkContext \n\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\").setAppName(\"Intro pyspark\"))\n\n\n# Verify SparkContext\nprint(sc)\n\n# Print Spark version\nprint(sc.version)","metadata":{},"execution_count":1,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"22/11/06 13:32:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n<SparkContext master=local[*] appName=Intro pyspark>\n3.3.0\n","output_type":"stream"}]}]}